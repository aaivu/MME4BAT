# -*- coding: utf-8 -*-
"""Hyperparameter tuning and comparing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1otlRg0C-KHFsB3LN8__bL8XsyVWSTw38
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime,date
from google.colab import files

import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval

from google.colab import drive
drive.mount('/content/drive')

path= '/content/drive/Shareddrives/MSc - Shiveswarran/Processed data/Nine_months_data/bus_stop_times_feature_added_all.csv'

df = pd.read_csv(path)

df

sns.set(style='whitegrid')
sns.set(rc={'figure.figsize':(16,12)})
sns.boxplot(x='bus_stop', y='dwell_time_in_seconds', data =  df)

df.columns

test = df[df['week_no']>=36]
train = df[df['week_no']<36]

Xtrain = train[['deviceid','bus_stop','day_of_week', 'Sunday/holiday', 'saturday','time_of_day','dt(w-1)', 'dt(w-2)', 'dt(w-3)', 'dt(t-1)','dt(t-2)', 'dt(n-1)', 'dt(n-2)', 'dt(n-3)','temp', 'precip','rt(n-1)']]
ytrain = train[['dwell_time_in_seconds']]

Xtest = test[['deviceid','bus_stop','day_of_week', 'Sunday/holiday', 'saturday','time_of_day','dt(w-1)', 'dt(w-2)', 'dt(w-3)', 'dt(t-1)','dt(t-2)', 'dt(n-1)', 'dt(n-2)', 'dt(n-3)','temp', 'precip','rt(n-1)']]
ytest = test[['dwell_time_in_seconds']]

space = {
    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),
    'max_depth' : hp.choice('max_depth', range(3,21,3)),
    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),
    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),     
    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]), 
    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100])
}

kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)

def objective(params):
    
    xgboost = xgb.XGBRegressor(seed=0, **params)
    score = cross_val_score(estimator=xgboost, 
                            X=Xtrain, 
                            y=ytrain, 
                            cv=kfold, 
                            scoring='neg_mean_squared_error', 
                            verbose = 1,
                            n_jobs=-1).mean()
    loss = (- score)**(1/2.0)
    return {'loss': loss, 'params': params, 'status': STATUS_OK}

best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 36, trials = Trials())

print(best)

print(space_eval(space, best))

xgboost_bo = xgb.XGBRegressor(seed=0, 
                           colsample_bytree=0.7, 
                           gamma=0.4, 
                           learning_rate=0.1, 
                           max_depth=6, 
                           reg_alpha=1e-05,
                           reg_lambda=100
                           ).fit(Xtrain,ytrain)

pred_xgb_bo = xgboost_bo.predict(Xtest)

rmse = np.sqrt(mean_squared_error(ytest, pred_xgb_bo)) 

print("RMSE (1): %f" % (rmse))

mape = mean_absolute_percentage_error(ytest, pred_xgb_bo)
print("MAPE (1): %f" % (mape)) 

mae = mean_absolute_error(ytest, pred_xgb_bo)
print("MAE (1): %f" % (mae)) 


r2 = r2_score(ytest, pred_xgb_bo)
print("r2 (1): %f" % (r2))

test.reset_index(drop = True, inplace = True)

pred_xgb_bo = pd.Series(pred_xgb_bo, name='XGBoost_Bay_Opt')

pred =test.merge(pred_xgb_bo,left_index=True, right_index=True)

pred = pred.sort_values(['trip_id', 'bus_stop'])

pred.reset_index(drop = True, inplace = True)

pred

pred['DateTime'] = pd.to_datetime(pred['date'] + ' ' + pred['arrival_time'])
ref_freq = '15min'
ix = pd.DatetimeIndex(pd.to_datetime(pred['DateTime'])).floor(ref_freq)
pred["DateTimeRef"] = ix

pred = pred[pred['DateTimeRef'].isin(pred_dwell['DateTimeRef'].tolist())]

path= '/content/drive/Shareddrives/MSc - Shiveswarran/Predicted values/predicted_dwell_times/predicted_dwell_times.csv'

pred_dwell = pd.read_csv(path)

pred_dwell

pred_dwell = pred_dwell.sort_values(['trip_id', 'bus_stop'])

pred_dwell =pred_dwell.merge(pred['XGBoost_Bay_Opt'],left_index=True, right_index=True)

pred_dwell

pip install bayesian-optimization

from bayes_opt import BayesianOptimization
import lightgbm as lgb
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')

def bayesion_opt_lgbm(X, y, init_iter=3, n_iters=7, random_state=11, seed = 101, num_iterations = 32):
  lgb_train = lgb.Dataset(Xtrain, ytrain)
  def lgb_r2_score(preds, lgb_train):
      labels = lgb_train.get_label()
      return 'r2', r2_score(labels, preds), True
  # Objective Function
  def hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight,learning_rate,lambda_l1,lambda_l2):
        
          params = {'application':'regression','num_iterations': num_iterations,
                    'early_stopping_round':50,
                    'metric':'lgb_r2_score'} # Default parameters
          params["num_leaves"] = int(round(num_leaves))
          params['feature_fraction'] = max(min(feature_fraction, 1), 0)
          params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)
          params['max_depth'] = int(round(max_depth))
          params['min_split_gain'] = min_split_gain
          params['min_child_weight'] = min_child_weight
          params['learning_rate'] = learning_rate
          params['lambda_l1'] = lambda_l1
          params['lambda_l2'] = lambda_l2
          cv_results = lgb.cv(params, lgb_train, nfold=5, seed=seed,categorical_feature=[], stratified=False,
                              verbose_eval =None, feval=lgb_r2_score)
          # print(cv_results)
          return np.max(cv_results['r2-mean'])
  # Domain space-- Range of hyperparameters 
  pds = {'num_leaves': (80, 100),
            'feature_fraction': (0.1, 0.9),
            'bagging_fraction': (0.8, 1),
            'max_depth': (17, 25),
            'min_split_gain': (0.001, 0.1),
            'min_child_weight': (10, 25),
            'learning_rate':(0.001,0.1),
            'lambda_l1':(0.0001,1),
            'lambda_l2':(0.0001,1)


            }

  # Surrogate model
  optimizer = BayesianOptimization(hyp_lgbm, pds, random_state=random_state)
                                    
  # Optimize
  optimizer.maximize(init_points=init_iter, n_iter=n_iters)

bayesion_opt_lgbm(Xtrain, ytrain, init_iter=5, n_iters=10, random_state=77, seed = 101, num_iterations = 100)

import lightgbm as lgb

params = {
    'task': 'train', 
    'boosting': 'gbdt',
    'objective': 'regression',
    'num_leaves': 88,
    'bagging_fraction':0.9,
    'feature_fraction':0.4,
    'lambda_l1': 0.7152,
    'lambda_l2': 0.8367,
    'max_depth ':19,
    'min_child_weight':14,
    'min_split_gain':0.07,    
    'learning_rate': 0.06,
    'metric': {'l2','l1'},
    'verbose': 0
}
Xtrain = train[['deviceid','bus_stop','day_of_week', 'Sunday/holiday', 'saturday','time_of_day','dt(w-1)', 'dt(w-2)', 'dt(w-3)', 'dt(t-1)','dt(t-2)', 'dt(n-1)', 'dt(n-2)', 'dt(n-3)','temp', 'precip','rt(n-1)']]
ytrain = train[['dwell_time_in_seconds']]

Xtest = test[['deviceid','bus_stop','day_of_week', 'Sunday/holiday', 'saturday','time_of_day','dt(w-1)', 'dt(w-2)', 'dt(w-3)', 'dt(t-1)','dt(t-2)', 'dt(n-1)', 'dt(n-2)', 'dt(n-3)','temp', 'precip','rt(n-1)']]
ytest = test[['dwell_time_in_seconds']]

lgb_train = lgb.Dataset(Xtrain, ytrain)
lgb_eval = lgb.Dataset(Xtest, ytest, reference=lgb_train)

model = lgb.train(params,
                 train_set=lgb_train,
                 valid_sets=lgb_eval,
                 early_stopping_rounds=30)


pred_lgb = model.predict(Xtest)

rmse = np.sqrt(mean_squared_error(ytest, pred_lgb)) 

print("RMSE (1): %f" % (rmse))

mape = mean_absolute_percentage_error(ytest, pred_lgb)
print("MAPE (1): %f" % (mape)) 

mae = mean_absolute_error(ytest, pred_lgb)
print("MAE (1): %f" % (mae)) 


r2 = r2_score(ytest, pred_lgb)
print("r2 (1): %f" % (r2))

test.reset_index(drop = True, inplace = True)

pred_lgb = pd.Series(pred_lgb, name='LightGBM_Bay_Opt')

pred =test.merge(pred_lgb,left_index=True, right_index=True)

pred['DateTime'] = pd.to_datetime(pred['date'] + ' ' + pred['arrival_time'])
ref_freq = '15min'
ix = pd.DatetimeIndex(pd.to_datetime(pred['DateTime'])).floor(ref_freq)
pred["DateTimeRef"] = ix

pred2 = pred[pred['DateTimeRef'].isin(pred_dwell['DateTimeRef'].tolist())]

pred_dwell = pred_dwell.sort_values(['trip_id', 'bus_stop'])

pred2 = pred2.sort_values(['trip_id', 'bus_stop'])

pred2.reset_index(drop = True, inplace = True)

pred_dwell

pred2

pred_dwell =pred_dwell.merge(pred2['LightGBM_Bay_Opt'],left_index=True, right_index=True)

def download_csv(data,filename):
  filename= filename + '.csv'
  data.to_csv(filename, encoding = 'utf-8-sig',index= False)
  files.download(filename)

download_csv(pred_dwell,'predicted_dwell_times')

print('results for convlstm:')
rmse = np.sqrt(mean_squared_error(pred_dwell['dwell_time_in_seconds'], pred_dwell['convlstm'])) 

print("RMSE (1): %f" % (rmse))

mape = mean_absolute_percentage_error(pred_dwell['dwell_time_in_seconds'], pred_dwell['convlstm'])
print("MAPE (1): %f" % (mape)) 

mae = mean_absolute_error(pred_dwell['dwell_time_in_seconds'], pred_dwell['convlstm'])
print("MAE (1): %f" % (mae)) 


r2 = r2_score(pred_dwell['dwell_time_in_seconds'], pred_dwell['convlstm'])
print("r2 (1): %f" % (r2))

plt.scatter(pred_dwell['dwell_time_in_seconds'], pred_dwell['XGBoost'])
plt.xlabel('True Values', fontsize=15)
plt.ylabel('XGB_Predictions', fontsize=15)
plt.plot([0,100,200,300,400,500,600],[0,100,200,300,400,500,600], 'r')
plt.plot()
plt.axis('equal')

plt.scatter(pred_dwell['dwell_time_in_seconds'], pred_dwell['XGBoost_Bay_Opt'])
plt.xlabel('True Values', fontsize=15)
plt.ylabel('XGB_Predictions', fontsize=15)
plt.plot([0,100,200,300,400,500,600],[0,100,200,300,400,500,600], 'r')
plt.plot()
plt.axis('equal')

plt.scatter(pred_dwell['dwell_time_in_seconds'], pred_dwell['LightGBM_Bay_Opt'])
plt.xlabel('True Values', fontsize=15)
plt.ylabel('LightGBM_Bay_Opt', fontsize=15)
plt.plot([0,100,200,300,400,500,600],[0,100,200,300,400,500,600], 'r')
plt.plot()
plt.axis('equal')

plt.scatter(pred_dwell['dwell_time_in_seconds'], pred_dwell['convlstm'])
plt.xlabel('True Values', fontsize=15)
plt.ylabel('convlstm_Predictions', fontsize=15)
plt.plot([0,100,200,300,400,500,600],[0,100,200,300,400,500,600], 'r')
plt.plot()
plt.axis('equal')

(pred_dwell['convlstm'] < 0).sum()

(pred_dwell['XGBoost_Bay_Opt'] < 0).sum()

(pred_dwell['LightGBM'] < 0).sum()

(pred_dwell['LightGBM_Bay_Opt'] < 0).sum()

(pred_dwell['XGBoost'] < 0).sum()



